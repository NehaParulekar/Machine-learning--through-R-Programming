---
title: "Project Part 2"
author: "Neha Parulekar"
output: word_document
---

# Analysis of Text Content 

`install.packages("dplyr");`
`install.packages("tm");`  `install.packages("Snowball");`
`install.packages("wordcloud");` 
`install.packages("cluster");`
```{r}
require(dplyr)
require(tm)
require(SnowballC)
require(wordcloud)
require(cluster)
```

#loading the data 
```{r}
getwd()
setwd("C:/Users/Neha/Desktop")
data = read.csv('Tweets.csv')
data = select(data, airline_sentiment, negativereason, airline, text)
head(data)

# Removing the @ 
  
data$text <- gsub("^@\\w+ *", "", data$text)
head(data)

# Dividing tweets based on positive and negative sentiment

posTweets <- subset(data, airline_sentiment == 'positive')
dim(posTweets)
NegTweets <- subset(data, airline_sentiment == 'negative')
dim(NegTweets)

# Removing these words seemed to be necessary as they are repeated a lot 
wordsToRemove = c('get', 'cant', 'can', 'now', 'just', 'will', 'dont', 'ive', 'got', 'much')

# analyse corpus 
analyseText = function(text_to_analyse){
    CorpusTranscript = Corpus(VectorSource(text_to_analyse))
    CorpusTranscript = tm_map(CorpusTranscript, content_transformer(tolower), lazy = T)
    CorpusTranscript = tm_map(CorpusTranscript, PlainTextDocument, lazy = T)
    CorpusTranscript = tm_map(CorpusTranscript, removePunctuation)
   CorpusTranscript = tm_map(CorpusTranscript, removeWords, wordsToRemove)  
    CorpusTranscript = tm_map(CorpusTranscript, removeWords, stopwords("english"))
    CorpusTranscript = DocumentTermMatrix(CorpusTranscript)
    CorpusTranscript = removeSparseTerms(CorpusTranscript, 0.97) # keeps a matrix 97% sparse
    CorpusTranscript = as.data.frame(as.matrix(CorpusTranscript))
    colnames(CorpusTranscript) = make.names(colnames(CorpusTranscript))
    
    return(CorpusTranscript)
}

# Analysing positive and Negative tweets
Nword <- analyseText(NegTweets$text)
dim(Nword)
Pword <- analyseText(posTweets$text)
dim(Pword)

# Determining the Frequency of negative words and creating a word cloud

Freq_Nword <- colSums(Nword)
Freq_Nword <- Freq_Nword[order(Freq_Nword, decreasing = T)]
head(Freq_Nword)
wordcloud(freq = as.vector(Freq_Nword), words = names(Freq_Nword), random.order = FALSE, random.color = FALSE, colors = brewer.pal(9, 'RdPu')[4:9] )


# Analysing Negative words generally mentioned in tweets 

analyseText2 = function(text_to_analyse){
  
    CorpusTranscript = Corpus(VectorSource(text_to_analyse))
    CorpusTranscript = tm_map(CorpusTranscript, content_transformer(tolower), lazy = T)
    CorpusTranscript = tm_map(CorpusTranscript, PlainTextDocument, lazy = T)
    CorpusTranscript = tm_map(CorpusTranscript, removePunctuation)
    CorpusTranscript = tm_map(CorpusTranscript, removeWords, wordsToRemove)
    CorpusTranscript = tm_map(CorpusTranscript, removeWords, stopwords("english"))
    CorpusTranscript = DocumentTermMatrix(CorpusTranscript)
    CorpusTranscript = removeSparseTerms(CorpusTranscript, 0.97) # keeps a matrix 97% sparse
    
    return(CorpusTranscript)
}

Nword <- analyseText2(NegTweets$text)
findAssocs(Nword, c("flight", 'customer', 'gate', 'phone'), .07)

```

# Further understanding the association with Clustering Analysis

```{r}
# hierarchical clustering
d = dist(t(as.matrix(Nword)), method = 'euclidean')
fit = hclust(d = d, method = 'ward.D')

# plotting the graph 
plot(fit, col = "#487AA1", col.main = "#45ADA8", col.lab = "#7C8071", main = 'Negative Sentiment', xlab = '', col.axis = "#F38630", lwd = 3, lty = 3, sub = "", hang = -1, axes = FALSE)

# add axis 
axis(side = 2, at = seq(0, 400, 100), col = "#F38630", labels = FALSE, lwd = 2)

# k-mean clustering 
d = dist(t(as.matrix(Nword)), method="euclidean")   
kfit = kmeans(d, 3)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0, cex = 0.4, main = 'Negative Sentiment')
```



  
