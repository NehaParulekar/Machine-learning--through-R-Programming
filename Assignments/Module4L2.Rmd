---
title: "Module4L2"
author: "Neha Parulekar"
date: "February 16, 2016"
output: word_document
---

# Additional packages needed
 
* If necessary install `ggplot2, cluster and amap` packages.  

`install.packages("ggplot2"); `  
`install.packages("cluster"); `  
`install.packages("amap"); `  
`install.packages("useful");  `  

```{r}
require(ggplot2)
require(cluster)
require(amap)
require(useful)
```

* Go to the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/) and find a dataset for clustering. Every student MUST use a different dataset so you MUST get approved for which you can going to use.

### Loading the dataset

```{r}
# set the working directory
setwd("C:/Users/Neha/Desktop")

# load the file
ProtienExpressionData <- read.csv("Data_Cortex_Nuclear.csv")

# Checking the data 
head(ProtienExpressionData)

# Removing the NA columns and columns without protien expression (last four cols)from the data 

RemoveColms <- c("MouseID","Genotype","Treatment","Behavior","class")
ModifiedProtienExpressionData <- ProtienExpressionData[, !(names(ProtienExpressionData) %in% RemoveColms)]

ModifiedProtienExpressionData <- ModifiedProtienExpressionData[complete.cases(ModifiedProtienExpressionData),]

#ModifiedProtienExpressionData
```

* Cluster some of your data using k-means, PAM and hierarchical clustering. Answer the following questions:

***Determining the k from Kmeans **
First determine the number of clusters then apply hartigans rule to get the K 
```{r}
# Determining the number of clusters
sos <- (nrow(ModifiedProtienExpressionData)-1)*sum(apply(ModifiedProtienExpressionData,2,var))
for(i in 2:10) sos[i] <- sum(kmeans(ModifiedProtienExpressionData, centers = i)$withinss)
plot(1:10, sos, type = "b", xlab = "Number of Clusters", ylab = "sum of squares")

# hartigans rule
best <- FitKMeans(ModifiedProtienExpressionData, max.clusters = 10, seed = 111)
PlotHartigan(best)

```

**From the graph I'm trying a K of 5,6,7,8**

```{r}
# Taking K value as 5
k <- 5
ProtienData.5.clust <- kmeans(ModifiedProtienExpressionData, k)
ProtienData.5.clust
```

```{r}
# Takiing K value as 6
k <- 6
ProtienData.6.clust <- kmeans(ModifiedProtienExpressionData, k)
ProtienData.6.clust
```

```{r}
# taking k value as 7
k <- 7
ProtienData.7.clust <- kmeans(ModifiedProtienExpressionData, k)
ProtienData.7.clust
```

**From the cluster sizes and Hartigans rule I think k = 7 gives a good clustering.** 
**Trying many number of trails**

```{r}
k = 7
trials <- 33
ProtienData.7.clust.33 <- kmeans(ModifiedProtienExpressionData, k, nstart = trials) 
ProtienData.7.clust.33
```


***Evaluating model performance.How do the clustering appaoches compare on the same data? **

```{r}
# look at the size of the clusters
ProtienData.7.clust.33$size

# look at the cluster centers
ProtienData.7.clust.33$centers

#ped <- unlist(ProtienExpressionData)

#plot(ped, col= ProtienData.7.clust.33$cluster)
```
The Cluster sizes are 98  60  80  57  57 117  83 with 33 trials. They are fairly good clusters.

***Generate and plot confusion matrices for the k-means and PAM. What do they tell you? **
We can say that the above confusion matrix shows the distribution of various mouse in the clusters based on clustering in genes. Since there is no small peculiar cluster, I did not inspect them specifically. We can also see a lot of 0 in mouses which indicates NA data.

```{r}
# Confusion Matrix for the K-means

ProtienDataCM<- ProtienExpressionData[complete.cases(ProtienExpressionData),]

cm.k <- table(ProtienDataCM$MouseID, ProtienData.7.clust.33$cluster)
dim(cm.k)


# Confusion Matrix for PAM
# just checking 
k = 7
trials <- 33
ProtienData.7.clust.33.PAM <- pam(ModifiedProtienExpressionData, k, keep.diss = T, keep.data = T) 
ProtienData.7.clust.33.PAM
ProtienDataCM<- ProtienExpressionData[complete.cases(ProtienExpressionData),]
cm.PAM <- table(ProtienDataCM$MouseID, ProtienData.7.clust.33.PAM$cluster)
dim(cm.PAM)
```

***Generate centroid plots against 1st two discriminant functions for k-means and PAM. What do they tell you? **
***Generate siloutte plots for PAM. What do they tell you? **  
the centroid and silholutte plots shows the spread and overlap of different clusters. Here we can see 7 clusters.
```{r}
# Centroid plot against 1st two discriminant function for k-means
clusplot(ModifiedProtienExpressionData, ProtienData.7.clust.33$cluster, color = T, shade = T, labels = 2, lines = 0)

```


```{r}
# k-medioids clustering in R
k = 7
ProtienData.7.clust.PAM <- pam(ModifiedProtienExpressionData, k, keep.diss = T, keep.data = T) 
ProtienData.7.clust.PAM

plot(ProtienData.7.clust.PAM, which.plots = 2)
```

***For the hierarchical clustering use all linkage methods (Single Link, Complete Link, Average Link,  Centroid and Minimum energy clustering) and generate dendograms. How they compare on the same data? **

```{r}
# Hierarchial Clustering
MPData.h.clust <- hclust(d = dist(ModifiedProtienExpressionData))
plot(MPData.h.clust)

# Using all the linkage methods
MPData.h.clust.si <- hclust(dist(ModifiedProtienExpressionData), method = 'single')
MPData.h.clust.co <- hclust(dist(ModifiedProtienExpressionData), method = 'complete')
MPData.h.clust.av <- hclust(dist(ModifiedProtienExpressionData), method = 'average')
MPData.h.clust.ce <- hclust(dist(ModifiedProtienExpressionData), method = 'centroid')
plot(MPData.h.clust.si, labels = F)
plot(MPData.h.clust.co, labels = F) 
plot(MPData.h.clust.av, labels = F)     
plot(MPData.h.clust.ce, labels = F)
```

***For the hierarchical clustering use both agglomerative and divisive clustering with a linkage method of your choice and generate dendograms. How they compare on the same data? **

***For the hierarchical clustering use centroid clustering and squared Euclidean distance and generate dendograms. How they compare on the same data? **
* We can say that Agglomerative is taking bottom-up approach. Divisive is taking top down approach.
* It can be seen that Average linkage method looks good, when we cut it at the height of 3.5 we get two clusters of equal halfs compared to complete, single and centroid.
* squared Euclidean distance is clearer that centroid clusters.

```{r}
## amap package required here
H_C <- hcluster(ModifiedProtienExpressionData, link = "ave")
plot(H_C)
plot(H_C, hang = -1)

# centroid clustering and squared Euclidean distance
h_c <- hclust(dist(ModifiedProtienExpressionData)^2, "cen")
plot(h_c)

# clustering with a linkage method of your choice and generate dendograms
h_C<- hcluster(ModifiedProtienExpressionData, method = "euc", link="ward", nbproc = 1, doubleprecision = TRUE)
plot(h_C, hang = -1)

```

